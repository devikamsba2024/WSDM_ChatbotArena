{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 86946,
          "databundleVersionId": 10131489,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devikamsba2024/WSDM_ChatbotArena/blob/main/WSDM_gemma_2_2b_Seq_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U transformers accelerate peft bitsandbytes evaluate\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:39:58.074468Z",
          "iopub.execute_input": "2025-04-25T00:39:58.074822Z",
          "iopub.status.idle": "2025-04-25T00:41:34.190818Z",
          "shell.execute_reply.started": "2025-04-25T00:39:58.074791Z",
          "shell.execute_reply": "2025-04-25T00:41:34.19012Z"
        },
        "id": "gT4aKhIPcyRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    GemmaTokenizerFast,\n",
        "    Gemma2ForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset as HFDataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import evaluate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:34.192182Z",
          "iopub.execute_input": "2025-04-25T00:41:34.192469Z",
          "iopub.status.idle": "2025-04-25T00:41:59.020558Z",
          "shell.execute_reply.started": "2025-04-25T00:41:34.192449Z",
          "shell.execute_reply": "2025-04-25T00:41:59.019812Z"
        },
        "id": "B40SkanycyRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to set all seeds for reproducibility\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"\n",
        "    Set seeds for reproducibility\n",
        "\n",
        "    Args:\n",
        "        seed: Seed value\n",
        "    \"\"\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.021328Z",
          "iopub.execute_input": "2025-04-25T00:41:59.021881Z",
          "iopub.status.idle": "2025-04-25T00:41:59.025942Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.021862Z",
          "shell.execute_reply": "2025-04-25T00:41:59.025229Z"
        },
        "id": "WZY_wj8UcyRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hugging Face token\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.02687Z",
          "iopub.execute_input": "2025-04-25T00:41:59.027161Z",
          "iopub.status.idle": "2025-04-25T00:41:59.298633Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.027133Z",
          "shell.execute_reply": "2025-04-25T00:41:59.297909Z"
        },
        "id": "uoda1jvKcyRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load_Data**\n",
        "\n",
        "1. Loads the data\n",
        "2. Select this percent of the subsample I want to select to train and test\n",
        "3. Make sure subsample has original class distribution\n"
      ],
      "metadata": {
        "id": "IDKJQZbNcyRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and sample data with preserved class distribution\n",
        "def load_data(file_path, sample_percentage=100, seed=42):\n",
        "    \"\"\"\n",
        "    Load the dataset and sample according to provided percentage while preserving class distribution\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the dataset CSV/Parquet\n",
        "        sample_percentage: Percentage of data to use (1-100)\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Sampled DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "\n",
        "    if sample_percentage < 100:\n",
        "        # Stratified sampling to preserve class distribution\n",
        "        if 'winner' in df.columns:\n",
        "            # Use stratified sampling\n",
        "            df = df.groupby('winner', group_keys=False).apply(\n",
        "                lambda x: x.sample(frac=sample_percentage/100, random_state=seed)\n",
        "            )\n",
        "        else:\n",
        "            # If no label column, use regular sampling\n",
        "            df = df.sample(frac=sample_percentage/100, random_state=seed)\n",
        "\n",
        "    print(f\"Loaded {len(df)} samples ({sample_percentage}% of original data)\")\n",
        "\n",
        "    # Print class distribution if available\n",
        "    if 'winner' in df.columns:\n",
        "        class_dist = df['winner'].value_counts(normalize=True) * 100\n",
        "        print(\"Class distribution:\")\n",
        "        for cls, pct in class_dist.items():\n",
        "            print(f\"  {cls}: {pct:.2f}%\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.300371Z",
          "iopub.execute_input": "2025-04-25T00:41:59.300605Z",
          "iopub.status.idle": "2025-04-25T00:41:59.310325Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.300588Z",
          "shell.execute_reply": "2025-04-25T00:41:59.309732Z"
        },
        "id": "DmH7ojNTcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **split_data**\n",
        "Splits the data into train, validation, test"
      ],
      "metadata": {
        "id": "rNoui8m_cyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split the data\n",
        "def split_data(df, train_size=0.7, val_size=0.1, test_size=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test sets\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        train_size: Proportion for training\n",
        "        val_size: Proportion for validation\n",
        "        test_size: Proportion for testing\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        train_df, val_df, test_df\n",
        "    \"\"\"\n",
        "    # First split: Train + Val vs Test\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=seed,\n",
        "                                            stratify=df['winner'] if 'winner' in df.columns else None)\n",
        "\n",
        "    # Second split: Train vs Val\n",
        "    relative_val_size = val_size / (train_size + val_size)\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=relative_val_size, random_state=seed,\n",
        "                                       stratify=train_val_df['winner'] if 'winner' in train_val_df.columns else None)\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df)*100:.2f}%)\")\n",
        "    print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df)*100:.2f}%)\")\n",
        "    print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.2f}%)\")\n",
        "\n",
        "    # Print class distribution for each split\n",
        "    if 'winner' in df.columns:\n",
        "        for name, split_df in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
        "            class_dist = split_df['winner'].value_counts(normalize=True) * 100\n",
        "            print(f\"{name} class distribution:\")\n",
        "            for cls, pct in class_dist.items():\n",
        "                print(f\"  {cls}: {pct:.2f}%\")\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.311151Z",
          "iopub.execute_input": "2025-04-25T00:41:59.311815Z",
          "iopub.status.idle": "2025-04-25T00:41:59.325721Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.31179Z",
          "shell.execute_reply": "2025-04-25T00:41:59.325144Z"
        },
        "id": "gHk-0Ho_cyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Truncate**\n",
        "smart_truncate will truncate the text based on the max_token set for the prompt and responses, but it will not truncate the end part; it will preserve the start and end but trim the middle.\n"
      ],
      "metadata": {
        "id": "66stjw27cyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform smart truncation on longer text\n",
        "def smart_truncate(text, tokenizer, max_tokens):\n",
        "    \"\"\"\n",
        "    Performs smart truncation by keeping start and end of text if it exceeds token limit\n",
        "\n",
        "    Args:\n",
        "        text: Text to truncate\n",
        "        tokenizer: Tokenizer to use\n",
        "        max_tokens: Maximum number of tokens allowed\n",
        "\n",
        "    Returns:\n",
        "        Truncated text\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n",
        "    if len(encoded['input_ids']) <= max_tokens:\n",
        "        return text\n",
        "\n",
        "    # Keep first and last parts\n",
        "    first_part_tokens = max_tokens // 2\n",
        "    last_part_tokens = max_tokens - first_part_tokens - 1  # -1 for the (snip) placeholder\n",
        "\n",
        "    # Get character indices from token offsets\n",
        "    if first_part_tokens > 0:\n",
        "        _, first_end_idx = encoded['offset_mapping'][first_part_tokens-1]\n",
        "    else:\n",
        "        first_end_idx = 0\n",
        "\n",
        "    if last_part_tokens > 0 and len(encoded['offset_mapping']) > last_part_tokens:\n",
        "        last_start_idx, _ = encoded['offset_mapping'][-last_part_tokens]\n",
        "    else:\n",
        "        last_start_idx = len(text)\n",
        "\n",
        "    # Combine first and last parts with (snip) in between\n",
        "    truncated_text = text[:first_end_idx] + \"\\n(snip)\\n\" + text[last_start_idx:]\n",
        "    return truncated_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.326422Z",
          "iopub.execute_input": "2025-04-25T00:41:59.326673Z",
          "iopub.status.idle": "2025-04-25T00:41:59.341171Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.326657Z",
          "shell.execute_reply": "2025-04-25T00:41:59.340346Z"
        },
        "id": "oJFRuPUJcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **preprocess_data**\n",
        "Before tokenizing, I make sure prompts and responses are truncated by a set limit with the help of smart_truncate()\n",
        "winner column is being encoded"
      ],
      "metadata": {
        "id": "Rqx3zObQcyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess dataset with smart truncation\n",
        "def preprocess_data(df, tokenizer, prompt_max_tokens=150, response_max_tokens=350):\n",
        "    \"\"\"\n",
        "    Preprocess data with smart truncation\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        tokenizer: Tokenizer\n",
        "        prompt_max_tokens: Maximum tokens for prompts\n",
        "        response_max_tokens: Maximum tokens for responses\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Fill NA values\n",
        "    for col in ['prompt', 'response_a', 'response_b']:\n",
        "        df[col] = df[col].fillna('')\n",
        "\n",
        "    # Apply smart truncation to each column\n",
        "    print(\"Applying smart truncation...\")\n",
        "    for col in ['prompt', 'response_a', 'response_b']:\n",
        "        max_tokens = prompt_max_tokens if col == 'prompt' else response_max_tokens\n",
        "        df[col] = df[col].apply(lambda x: smart_truncate(x, tokenizer, max_tokens))\n",
        "\n",
        "    # Encode winner labels\n",
        "    if 'winner' in df.columns:\n",
        "        df['winner_encoded'] = df['winner'].map({'model_a': 0, 'model_b': 1})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.342229Z",
          "iopub.execute_input": "2025-04-25T00:41:59.342505Z",
          "iopub.status.idle": "2025-04-25T00:41:59.355278Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.342476Z",
          "shell.execute_reply": "2025-04-25T00:41:59.354514Z"
        },
        "id": "4eSDv_LScyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenize**\n",
        "\n",
        "* tokenize_with_special_tokens()- tokenize's the prompt, response_a and response_b seperately after they are truncated.\n",
        "* As LLMs expect the same length of text for every row, some of the input will be padded, but those can be ignored by the model with an attention mask.All tokens, attention mask,winner label are stored in dictionary.\n"
      ],
      "metadata": {
        "id": "2iHWdrr_cyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize inputs with special tokens\n",
        "def tokenize_with_special_tokens(df, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Tokenize inputs with special tokens\n",
        "\n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        tokenizer: Tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with input_ids, attention_mask and labels\n",
        "    \"\"\"\n",
        "    # Create texts with special tokens\n",
        "    prompts = [\"<prompt>: \" + p for p in df['prompt']]\n",
        "    response_a = [\"\\n\\n<response_a>: \" + r for r in df['response_a']]\n",
        "    response_b = [\"\\n\\n<response_b>: \" + r for r in df['response_b']]\n",
        "\n",
        "    # Combine all parts into a single text\n",
        "    texts = [p + ra + rb for p, ra, rb in zip(prompts, response_a, response_b)]\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(texts, max_length=max_length, truncation=True)\n",
        "\n",
        "    # Create dataset dictionary\n",
        "    dataset_dict = {\n",
        "        'input_ids': tokenized['input_ids'],\n",
        "        'attention_mask': tokenized['attention_mask']\n",
        "    }\n",
        "\n",
        "    # Add labels if available\n",
        "    if 'winner_encoded' in df.columns:\n",
        "        dataset_dict['labels'] = df['winner_encoded'].tolist()\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.356107Z",
          "iopub.execute_input": "2025-04-25T00:41:59.356419Z",
          "iopub.status.idle": "2025-04-25T00:41:59.373156Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.356387Z",
          "shell.execute_reply": "2025-04-25T00:41:59.372428Z"
        },
        "id": "Y0j0mkfYcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Huggingface Datasets**\n",
        "Converting the dictionary I created earlier while tokenizing are transforming into HuggingFace Datasets"
      ],
      "metadata": {
        "id": "GA-GxN8xcyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create HuggingFace datasets\n",
        "def create_hf_datasets(train_df, val_df, test_df, tokenizer, max_length=512):\n",
        "    \"\"\"\n",
        "    Create HuggingFace datasets from DataFrames\n",
        "\n",
        "    Args:\n",
        "        train_df, val_df, test_df: DataFrames for each split\n",
        "        tokenizer: Tokenizer\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        HuggingFace datasets\n",
        "    \"\"\"\n",
        "    train_dict = tokenize_with_special_tokens(train_df, tokenizer, max_length)\n",
        "    val_dict = tokenize_with_special_tokens(val_df, tokenizer, max_length)\n",
        "    test_dict = tokenize_with_special_tokens(test_df, tokenizer, max_length)\n",
        "\n",
        "    hf_train_dataset = HFDataset.from_dict(train_dict)\n",
        "    hf_val_dataset = HFDataset.from_dict(val_dict)\n",
        "    hf_test_dataset = HFDataset.from_dict(test_dict)\n",
        "\n",
        "    return hf_train_dataset, hf_val_dataset, hf_test_dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.373934Z",
          "iopub.execute_input": "2025-04-25T00:41:59.374195Z",
          "iopub.status.idle": "2025-04-25T00:41:59.388205Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.374176Z",
          "shell.execute_reply": "2025-04-25T00:41:59.387542Z"
        },
        "id": "1jitRWIIcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **init_tokenizer**\n",
        "initializes the tokeniszer which is GemmaTokenizerFast"
      ],
      "metadata": {
        "id": "SMB5FSuLcyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_tokenizer(model_path,use_auth_token=True):\n",
        "    \"\"\"\n",
        "    Initialize the GemmaTokenizerFast\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the model/tokenizer\n",
        "\n",
        "    Returns:\n",
        "        tokenizer\n",
        "    \"\"\"\n",
        "    tokenizer = GemmaTokenizerFast.from_pretrained(\n",
        "        model_path,\n",
        "        add_eos_token=True,\n",
        "        padding_side=\"right\",\n",
        "        use_auth_token=use_auth_token\n",
        "         # Add this to specify we're loading from local files\n",
        "    )\n",
        "    return tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.388937Z",
          "iopub.execute_input": "2025-04-25T00:41:59.389231Z",
          "iopub.status.idle": "2025-04-25T00:41:59.399308Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.389208Z",
          "shell.execute_reply": "2025-04-25T00:41:59.398574Z"
        },
        "id": "txSpJ2hVcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **init_model**\n",
        "initializing the model with the following\n",
        "\n",
        "* I have added the model to Kaggle Gemma 2 2b is already available,  so just loaded the model, which is already available.\n",
        "* Using prepare_model_for_kbit_training to quantize the model.\n",
        "* As part of PEFT(Parameter Efficient Finetuning) i am using Low Rank Adaption technique where i applying to layers 6 to 26( First 6 layers are being freezed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZK4T_5dGcyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize model with LoRA and layer freezing\n",
        "def init_model(model_name=\"google/gemma-2-2b\", num_labels=2, quantize=True, num_layers=26, freeze_layers=6,use_auth_token=True):\n",
        "    \"\"\"\n",
        "    Initialize Gemma2ForSequenceClassification with LoRA parameters and layer freezing\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the model\n",
        "        num_labels: Number of output classes\n",
        "        quantize: Whether to use quantization\n",
        "        num_layers: Total number of layers in the model\n",
        "        freeze_layers: Number of layers to freeze\n",
        "\n",
        "    Returns:\n",
        "        LoRA-configured model\n",
        "    \"\"\"\n",
        "    # Load the base model\n",
        "    model = Gemma2ForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=False,  # Important for training\n",
        "        use_auth_token=use_auth_token,\n",
        "        problem_type=\"single_label_classification\",\n",
        "\n",
        "    )\n",
        "\n",
        "    # Prepare model for k-bit training if quantizing\n",
        "    if quantize:\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA configuration with layer targeting\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=32,  # Rank\n",
        "        lora_alpha=64,  # Alpha parameter for LoRA scaling\n",
        "        lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # Target attention mechanisms\n",
        "        layers_to_transform=[i for i in range(num_layers) if i >= freeze_layers],  # Only apply to non-frozen layers\n",
        "        bias='none',\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.400139Z",
          "iopub.execute_input": "2025-04-25T00:41:59.400394Z",
          "iopub.status.idle": "2025-04-25T00:41:59.413454Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.400372Z",
          "shell.execute_reply": "2025-04-25T00:41:59.412924Z"
        },
        "id": "CzQGBOZpcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train Model**\n",
        "Using the trainer API to train the model and passing all the hyperparameters, data, and metrics I want the model to get evaluated.\n",
        "Saves the best model that has the best Validation Accuracy"
      ],
      "metadata": {
        "id": "dDpLUCaFcyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dataset, val_dataset, output_dir, model, learning_rate=5e-5, batch_size=1, num_epochs=3,\n",
        "                grad_accum_steps=4, eval_steps=None, save_steps=200):\n",
        "    \"\"\"\n",
        "    Train the model with given parameters\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_dataset, val_dataset: Training and validation datasets\n",
        "        output_dir: Directory to save model artifacts\n",
        "        learning_rate: Learning rate\n",
        "        batch_size: Batch size\n",
        "        num_epochs: Number of training epochs\n",
        "        grad_accum_steps: Gradient accumulation steps\n",
        "        eval_steps: Evaluation steps (if None, evaluate every epoch)\n",
        "        save_steps: Model saving steps\n",
        "\n",
        "    Returns:\n",
        "        Trained model and training metrics\n",
        "    \"\"\"\n",
        "    # Compute metrics function\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return {'accuracy': accuracy_score(y_true=labels, y_pred=predictions)}\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        # Evaluate during training to track progress\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=eval_steps if eval_steps else 500,  # Default to evaluate every 100 steps\n",
        "        # Save checkpoints\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=save_steps,\n",
        "        save_total_limit=3,\n",
        "        # Load best model at end\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        greater_is_better=True,\n",
        "        # Logging settings\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=500,\n",
        "        logging_first_step=True,  # Log the first step to see initial metrics\n",
        "        # Report training loss\n",
        "        report_to=\"tensorboard\",\n",
        "        # Precision settings\n",
        "        fp16=True,\n",
        "        bf16=False,\n",
        "        # Other settings\n",
        "        gradient_accumulation_steps=grad_accum_steps,\n",
        "        warmup_ratio=0.1,  # Increased from 0.01 to help with training stability\n",
        "        lr_scheduler_type=\"cosine\",  # Changed from linear to cosine for better learning rate decay\n",
        "        # Add label smoothing to help with overconfidence\n",
        "        label_smoothing_factor=0.1,\n",
        "        # Group by length for more efficient batching\n",
        "        group_by_length=True,\n",
        "        # Add some noise to avoid overfitting\n",
        "        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n",
        "    )\n",
        "\n",
        "    # Initialize trainer with early stopping\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[\n",
        "            EarlyStoppingCallback(early_stopping_patience=3),\n",
        "            # Add a custom callback to track training accuracy\n",
        "            TrainCallback(),\n",
        "        ],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Dynamic padding\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the best model\n",
        "    trainer.save_model(f\"{output_dir}/best_model\")\n",
        "\n",
        "    return trainer\n",
        "\n",
        "# Custom callback to track training metrics\n",
        "class TrainCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to track training metrics\"\"\"\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Called each time trainer logs metrics\"\"\"\n",
        "        logs = logs or {}\n",
        "\n",
        "        # Extract training accuracy if available\n",
        "        if 'loss' in logs:\n",
        "            print(f\"Step {state.global_step}: Training Loss: {logs['loss']}\")\n",
        "\n",
        "        # Print evaluation metrics when they exist\n",
        "        if 'eval_accuracy' in logs:\n",
        "            print(f\"Step {state.global_step}: Eval Accuracy: {logs['eval_accuracy']}, Eval Loss: {logs.get('eval_loss', 'N/A')}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:56:55.808281Z",
          "iopub.execute_input": "2025-04-25T00:56:55.808582Z",
          "iopub.status.idle": "2025-04-25T00:56:55.819417Z",
          "shell.execute_reply.started": "2025-04-25T00:56:55.808561Z",
          "shell.execute_reply": "2025-04-25T00:56:55.818397Z"
        },
        "id": "9ig3qQ8JcyRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing**\n",
        "Tests the Finetuned model with test data"
      ],
      "metadata": {
        "id": "icbOqK32cyRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate on test set\n",
        "def evaluate_model(trainer, test_dataset):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set\n",
        "\n",
        "    Args:\n",
        "        trainer: Trained Trainer object\n",
        "        test_dataset: Test dataset\n",
        "\n",
        "    Returns:\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Evaluate on test set\n",
        "    test_results = trainer.evaluate(test_dataset)\n",
        "    print(f\"Test set results: {test_results}\")\n",
        "    return test_results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.434798Z",
          "iopub.execute_input": "2025-04-25T00:41:59.435091Z",
          "iopub.status.idle": "2025-04-25T00:41:59.446201Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.435075Z",
          "shell.execute_reply": "2025-04-25T00:41:59.445541Z"
        },
        "id": "aZdyLEdfcyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# 1. Set seed for reproducibility\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.44694Z",
          "iopub.execute_input": "2025-04-25T00:41:59.447191Z",
          "iopub.status.idle": "2025-04-25T00:41:59.464115Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.447168Z",
          "shell.execute_reply": "2025-04-25T00:41:59.463494Z"
        },
        "id": "FjBL1TFocyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load and sample data\n",
        "data = load_data(\"/content/train.parquet\", sample_percentage=100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:41:59.46479Z",
          "iopub.execute_input": "2025-04-25T00:41:59.465027Z",
          "iopub.status.idle": "2025-04-25T00:42:01.556367Z",
          "shell.execute_reply.started": "2025-04-25T00:41:59.465008Z",
          "shell.execute_reply": "2025-04-25T00:42:01.555589Z"
        },
        "id": "nvWANd3CcyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split the data\n",
        "train_df, val_df, test_df = split_data(data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:42:01.557156Z",
          "iopub.execute_input": "2025-04-25T00:42:01.557356Z",
          "iopub.status.idle": "2025-04-25T00:42:01.577781Z",
          "shell.execute_reply.started": "2025-04-25T00:42:01.557342Z",
          "shell.execute_reply": "2025-04-25T00:42:01.576933Z"
        },
        "id": "s1k2KgQJcyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2-2b\"  # HuggingFace model name\n",
        "tokenizer = init_tokenizer(model_name, use_auth_token=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:42:01.578811Z",
          "iopub.execute_input": "2025-04-25T00:42:01.579065Z",
          "iopub.status.idle": "2025-04-25T00:42:04.044761Z",
          "shell.execute_reply.started": "2025-04-25T00:42:01.579045Z",
          "shell.execute_reply": "2025-04-25T00:42:04.044178Z"
        },
        "id": "N2-CL1NycyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Preprocess data with smart truncation\n",
        "train_df = preprocess_data(train_df, tokenizer)\n",
        "val_df = preprocess_data(val_df, tokenizer)\n",
        "test_df = preprocess_data(test_df, tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:42:04.045493Z",
          "iopub.execute_input": "2025-04-25T00:42:04.045688Z",
          "iopub.status.idle": "2025-04-25T00:42:20.397504Z",
          "shell.execute_reply.started": "2025-04-25T00:42:04.045673Z",
          "shell.execute_reply": "2025-04-25T00:42:20.396914Z"
        },
        "id": "evgo9Zj1cyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_model(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        "    quantize=True,\n",
        "    num_layers=26,\n",
        "    freeze_layers=6,\n",
        "    use_auth_token=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:42:20.398224Z",
          "iopub.execute_input": "2025-04-25T00:42:20.398486Z",
          "iopub.status.idle": "2025-04-25T00:43:09.730876Z",
          "shell.execute_reply.started": "2025-04-25T00:42:20.398463Z",
          "shell.execute_reply": "2025-04-25T00:43:09.730042Z"
        },
        "id": "6IG9C3pecyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create HuggingFace datasets\n",
        "hf_train_dataset, hf_val_dataset, hf_test_dataset = create_hf_datasets(\n",
        "    train_df, val_df, test_df, tokenizer, max_length=512\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:43:09.731662Z",
          "iopub.execute_input": "2025-04-25T00:43:09.731927Z",
          "iopub.status.idle": "2025-04-25T00:43:15.523107Z",
          "shell.execute_reply.started": "2025-04-25T00:43:09.73191Z",
          "shell.execute_reply": "2025-04-25T00:43:15.522517Z"
        },
        "id": "tKBWtJNFcyRb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train model\n",
        "output_dir = \"./model_output\"\n",
        "trainer = train_model(\n",
        "    hf_train_dataset,\n",
        "    hf_val_dataset,\n",
        "    output_dir,\n",
        "    model,\n",
        "    learning_rate=2e-5,\n",
        "    batch_size=1,\n",
        "    num_epochs=1,\n",
        "    grad_accum_steps=8,\n",
        "    eval_steps=500,  # Evaluate every 200 steps\n",
        "    save_steps=500,  # Save every 200 steps\n",
        "\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:57:05.722803Z",
          "iopub.execute_input": "2025-04-25T00:57:05.723317Z"
        },
        "id": "N3GHxWobcyRb",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluate on test set\n",
        "test_results = evaluate_model(trainer, hf_test_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-25T00:43:22.803295Z",
          "iopub.status.idle": "2025-04-25T00:43:22.803607Z",
          "shell.execute_reply.started": "2025-04-25T00:43:22.803439Z",
          "shell.execute_reply": "2025-04-25T00:43:22.803455Z"
        },
        "id": "xt67RJxAcyRb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}